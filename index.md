---
layout: home
profile_picture:
  src: /assets/img/profile_pic_sattar.jpg
  alt: sattar vakili
---

# Research Interests

<ul>
  <li> Machine learning and AI</li>
  <li> Diffusion models</li>
  <li> Sequential decision making, bandit and RL</li>
  <li> Kernel methods, Gaussian processes and Bayesian optimisation</li>
<ul>



<span style="color:white">Sattar Vakili</span>

<span style="color:white">Machine Learning, Bandit, Reinforcement learning, kernel methods, Google Scholar, LinkedIn, Cornell, MediaTek, MediaTek Research, Cambridge </span>


# Selected Projects

## Kernel-based RL

<img src="{{ "/assets/img/RL_img.png" | absolute_url }}" alt="{{ Image }}"  style="width:400px;height:400px;" />

Reinforcement Learning (RL) has shown great empirical success in various settings with complex models and large state-action spaces. However, the existing analytical results typically focus on settings with a small number of state-actions or simple models, such as linearly modeled state-action value functions. To derive RL policies that efficiently handle large state-action spaces with more general value functions, some recent works have explored nonlinear function approximation using kernel ridge regression. In this talk, we examine existing results in this RL setting, analytical tools, their limitations and some open problems. Moreover, we introduce a kernel based optimistic least-squares value iteration policy that achieves order optimal regret bounds for a common class of kernels. 

